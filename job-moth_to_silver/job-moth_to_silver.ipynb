{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%%configure\n{\n   \"--datalake-formats\": \"iceberg\",\n    \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO --conf spark.sql.catalog.glue_catalog.warehouse=file:///tmp/spark-warehouse --conf spark.sql.defaultCatalog=glue_catalog\"\n}  ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nThe following configurations have been updated: {'--datalake-formats': 'iceberg', '--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO --conf spark.sql.catalog.glue_catalog.warehouse=file:///tmp/spark-warehouse --conf spark.sql.defaultCatalog=glue_catalog'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nfrom datetime import datetime\nimport pandas as pd\nfrom pyspark.sql.functions import to_timestamp\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: b27f5a4d-6c7d-4afa-9343-6c635dbb4cfd\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--datalake-formats iceberg\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO --conf spark.sql.catalog.glue_catalog.warehouse=file:///tmp/spark-warehouse --conf spark.sql.defaultCatalog=glue_catalog\nWaiting for session b27f5a4d-6c7d-4afa-9343-6c635dbb4cfd to get into ready status...\nSession b27f5a4d-6c7d-4afa-9343-6c635dbb4cfd has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from datetime import datetime\nimport pandas as pd\nfrom pyspark.sql.functions import to_timestamp\nfrom awsglue import DynamicFrame",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def sparkSqlQuery(glueContext, query, mapping, transformation_ctx) -> DynamicFrame:\n    for alias, frame in mapping.items():\n        frame.toDF().createOrReplaceTempView(alias)\n    result = spark.sql(query)\n    return DynamicFrame.fromDF(result, glueContext, transformation_ctx)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf_kobo_bronze = glueContext.create_dynamic_frame.from_catalog(database=\"tgsn_bronze\", table_name=\"kobo_moth\", transformation_ctx=\"dyf_kobo_bronze\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "query_to_silver = '''\nSELECT \n_id as ID,\nto_timestamp(start) as start_date,\nto_timestamp(end) as end_date,\ndeviceid,\ninterviewer_username,\npre_interview_instructions,\ncamp_name,\ngps_coordinates,\nrespondent_gender,\ninterview_introduction,\nconsent_understood,\nend_interview_1,\nconsent_request,\naccuracy_request,\nconsent_signature,\nto_date(consent_date) as consent_date,\nconsent_rejection_reason,\nend_interview_2,\nend_interview_3,\nrespondent_name_first,\nrespondent_name_last,\nrespondent_name_other,\ncampidcard_yes_no,\ncampidcard_cardnumber,\ncampidcard_caseid,\nto_date(respondent_birth_date) as respondent_birth_date,\nrespondent_birth_country,\nrespondent_birth_place,\nrespondent_nationality_first,\nrespondent_nationality_second,\nrespondent_nationality_differentatbirth_yes_no,\nrespondent_nationality_atbirth,\neducation,\nprewar_residence_country,\nprewar_residence_province,\nprewar_residence_place,\nprewar_occupation_yes_no,\nprewar_occupation,\ncast(departure_year as int) as departure_year,\ndeparture_month,\ncast(camp_arrival_year as int) as camp_arrival_year,\ncamp_arrival_month,\ncamp_occupation_yes_no,\ncamp_occupation,\ncamp_occupation_other,\nhealth_physical_treated,\nhealth_physical_untreated,\nhealth_physical_symptoms,\nhealth_psychological_treated,\nhealth_psychological_untreated,\nhealth_psychological_symptoms,\npss_interest,\nfamily_introduction,\nmarital_status,\nwives_number,\nchildren_yes_no,\nchildren_number,\nother_relatives_yes_no,\nrelatives_number,\nreintegration_preference,\nreintegration_preference_reason,\nphoto_question,\nphoto_respondent,\nphoto_children,\ninterview_conclusion,\npost_interview_instructions,\nrespondent_comfort,\nrespondent_comprehension,\ninterviewer_feedback_notes,\nfinal_instructions,\nto_date(_submission_time) as submission_time,\n_submitted_by as submitted_by,\n_attachments as attachments\nFROM kobo_moth_bronze\nWHERE to_date(_submission_time) >= current_date()\norder by _id\n'''",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf_kobo_to_silver = sparkSqlQuery(glueContext, query = query_to_silver, mapping = {\"kobo_moth_bronze\":dyf_kobo_bronze}, transformation_ctx = \"df_kobo_to_silver\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_kobo_to_silver = dyf_kobo_to_silver.toDF()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_kobo_to_silver.select('id','start_date', 'respondent_name_first','submission_time').show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+--------------------+---------------------+---------------+\n| id|          start_date|respondent_name_first|submission_time|\n+---+--------------------+---------------------+---------------+\n| 83|2025-01-21 14:21:...|                 Amir|     2025-01-21|\n+---+--------------------+---------------------+---------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Script generated for node Amazon S3\nadditional_options = {}\ntables_collection = spark.catalog.listTables(\"tgsn_silver\")\ntable_names_in_db = [table.name for table in tables_collection]\ntable_exists = \"kobo_moth\" in table_names_in_db\nif table_exists:\n    df_kobo_to_silver.sortWithinPartitions(\"submission_time\") \\\n        .writeTo(\"glue_catalog.tgsn_silver.kobo_moth\") \\\n        .tableProperty(\"format-version\", \"2\") \\\n        .tableProperty(\"location\", \"s3://tgsn-silver-bucket/kobo/Moth/meta/tgsn_silver/kobo_moth\") \\\n        .tableProperty(\"write.parquet.compression-codec\", \"gzip\") \\\n        .options(**additional_options) \\\n.append()\nelse:\n    df_kobo_to_silver.writeTo(\"glue_catalog.tgsn_silver.kobo_moth\") \\\n        .tableProperty(\"format-version\", \"2\") \\\n        .tableProperty(\"location\", \"s3://tgsn-silver-bucket/kobo/Moth/meta/tgsn_silver/kobo_moth\") \\\n        .tableProperty(\"write.parquet.compression-codec\", \"gzip\") \\\n        .options(**additional_options) \\\n        .partitionedBy(\"submission_time\") \\\n.create()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		}
	]
}