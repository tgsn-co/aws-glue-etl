{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nfrom datetime import datetime\nimport pandas as pd\nfrom pyspark.sql.functions import to_timestamp, to_date\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: 5d508551-3801-411c-a3ed-8a04a9e93ad6\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session 5d508551-3801-411c-a3ed-8a04a9e93ad6 to get into ready status...\nSession 5d508551-3801-411c-a3ed-8a04a9e93ad6 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Read CSV files from KoBo landing layer\ndyf_kobo_landing = glueContext.create_dynamic_frame.from_options(\n    format_options={\"quoteChar\": \"\\\"\", \"withHeader\": True, \"separator\": \"|\"}, \n    connection_type=\"s3\", format=\"csv\", \n    connection_options={\"paths\": [\"s3://tgsn-landing/kobo_data_landing/\"], \"recurse\": True}, \n    transformation_ctx=\"dyf_kobo_landing\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf_kobo_landing.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- start: string\n|-- end: string\n|-- deviceid: string\n|-- interviewer_username: string\n|-- pre_interview_instructions: string\n|-- camp_name: string\n|-- gps_coordinates: string\n|-- respondent_gender: string\n|-- interview_introduction: string\n|-- consent_understood: string\n|-- end_interview_1: string\n|-- consent_request: string\n|-- accuracy_request: string\n|-- consent_signature: string\n|-- consent_date: string\n|-- consent_rejection_reason: string\n|-- end_interview_2: string\n|-- end_interview_3: string\n|-- respondent_name_first: string\n|-- respondent_name_last: string\n|-- respondent_name_other: string\n|-- campidcard_yes_no: string\n|-- campidcard_cardnumber: string\n|-- campidcard_caseid: string\n|-- respondent_birth_date: string\n|-- respondent_birth_country: string\n|-- respondent_birth_place: string\n|-- respondent_nationality_first: string\n|-- respondent_nationality_second: string\n|-- respondent_nationality_differentatbirth_yes_no: string\n|-- respondent_nationality_atbirth: string\n|-- education: string\n|-- prewar_residence_country: string\n|-- prewar_residence_province: string\n|-- prewar_residence_place: string\n|-- prewar_occupation_yes_no: string\n|-- prewar_occupation: string\n|-- departure_year: string\n|-- departure_month: string\n|-- camp_arrival_year: string\n|-- camp_arrival_month: string\n|-- camp_occupation_yes_no: string\n|-- camp_occupation: string\n|-- camp_occupation_other: string\n|-- health_physical_treated: string\n|-- health_physical_untreated: string\n|-- health_physical_symptoms: string\n|-- health_psychological_treated: string\n|-- health_psychological_untreated: string\n|-- health_psychological_symptoms: string\n|-- pss_interest: string\n|-- family_introduction: string\n|-- marital_status: string\n|-- wives_number: string\n|-- children_yes_no: string\n|-- children_number: string\n|-- other_relatives_yes_no: string\n|-- relatives_number: string\n|-- reintegration_preference: string\n|-- reintegration_preference_reason: string\n|-- photo_question: string\n|-- photo_respondent: string\n|-- photo_children: string\n|-- interview_conclusion: string\n|-- post_interview_instructions: string\n|-- respondent_comfort: string\n|-- respondent_comprehension: string\n|-- interviewer_feedback_notes: string\n|-- final_instructions: string\n|-- _id: string\n|-- formhub/uuid: string\n|-- __version__: string\n|-- meta/instanceID: string\n|-- _xform_id_string: string\n|-- _uuid: string\n|-- _status: string\n|-- _geolocation: string\n|-- _submission_time: string\n|-- _validation_status: string\n|-- _submitted_by: string\n|-- _attachments: string\n|-- _notes: string\n|-- _tags: string\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Convert the DynamicFrame to a Spark DataFrame\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "spark_df_kobo = dyf_kobo_landing.toDF()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df_kobo = spark_df_kobo.withColumn(\"_submission_time_date\",to_date(\"_submission_time\"))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df_kobo_today = spark_df_kobo.where(spark_df_kobo['_submission_time_date'] >= datetime.today().strftime('%Y-%m-%d'))\n#spark_df_kobo_today = spark_df_kobo",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df_kobo_today.select('respondent_name_first', '_submission_time','_submission_time_date').show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------------------+-------------------+---------------------+\n|respondent_name_first|   _submission_time|_submission_time_date|\n+---------------------+-------------------+---------------------+\n|                 John|2024-11-21T14:58:01|           2024-11-21|\n|              Manuela|2024-11-21T15:16:23|           2024-11-21|\n|                  Ann|2024-11-22T15:06:18|           2024-11-22|\n|                 Ezra|2024-11-25T14:25:44|           2024-11-25|\n|                  Rex|2024-11-26T14:20:55|           2024-11-26|\n|                  Rob|2024-11-28T15:27:04|           2024-11-28|\n|                     |2024-11-29T10:35:00|           2024-11-29|\n|                     |2024-12-02T09:50:24|           2024-12-02|\n|                 Tess|2024-12-03T09:18:28|           2024-12-03|\n|                 Lulu|2024-12-05T09:13:13|           2024-12-05|\n|              Maymuna|2024-12-05T09:15:26|           2024-12-05|\n+---------------------+-------------------+---------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "from awsglue.dynamicframe import DynamicFrame\n\n# Convert Spark DataFrame to Glue DynamicFrame \ndyf_kobo = DynamicFrame.fromDF(spark_df_kobo_today, glueContext)\n\ns3output = glueContext.getSink(\n  path=\"s3://tgsn-bronze/kobo/Moth/\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=['_submission_time_date'],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\"\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"tgsn_bronze\", catalogTableName=\"kobo_moth\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(dyf_kobo)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f68aa2520b0>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}